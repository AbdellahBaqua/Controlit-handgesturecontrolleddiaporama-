

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Professor Gesture &amp; Speech Control App &mdash; Controlit today documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=9fdc9bb0"></script>
      <script src="_static/doctools.js?v=888ff710"></script>
      <script src="_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Usage Guide" href="usage.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Controlit
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage Guide</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Professor Gesture &amp; Speech Control App</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#external-files-models">External Files &amp; Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#architecture-program-flow">Architecture &amp; Program Flow</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#state-1-searching-professor-initial">State 1: <cite>SEARCHING_PROFESSOR_INITIAL</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="#state-2-tracking-professor">State 2: <cite>TRACKING_PROFESSOR</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="#state-3-reacquiring-professor">State 3: <cite>REACQUIRING_PROFESSOR</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="#speech-recognition-sub-system">Speech Recognition Sub-System</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-function-reference">Module &amp; Function Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#speech-recognition">Speech Recognition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#get_keyword"><code class="docutils literal notranslate"><span class="pre">get_keyword()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#execute_speech_action"><code class="docutils literal notranslate"><span class="pre">execute_speech_action()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#image-processing-landmark-calculation">Image Processing &amp; Landmark Calculation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#calc_bounding_rect"><code class="docutils literal notranslate"><span class="pre">calc_bounding_rect()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#calc_landmark_list"><code class="docutils literal notranslate"><span class="pre">calc_landmark_list()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pre_process_landmark"><code class="docutils literal notranslate"><span class="pre">pre_process_landmark()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-logging-debug-drawing">Data Logging &amp; Debug Drawing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#logging_csv"><code class="docutils literal notranslate"><span class="pre">logging_csv()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#draw_landmarks"><code class="docutils literal notranslate"><span class="pre">draw_landmarks()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#draw_info_text"><code class="docutils literal notranslate"><span class="pre">draw_info_text()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#draw_info"><code class="docutils literal notranslate"><span class="pre">draw_info()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#main-application">Main Application</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#main"><code class="docutils literal notranslate"><span class="pre">main()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#modes-command-reference">Modes &amp; Command Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#in-app-controls">In-App Controls</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gesture-voice-commands">Gesture &amp; Voice Commands</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Controlit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Professor Gesture &amp; Speech Control App</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/app.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="professor-gesture-speech-control-app">
<h1>Professor Gesture &amp; Speech Control App<a class="headerlink" href="#professor-gesture-speech-control-app" title="Permalink to this heading"></a></h1>
<section id="external-files-models">
<h2>External Files &amp; Models<a class="headerlink" href="#external-files-models" title="Permalink to this heading"></a></h2>
<p>The application relies on several external files that must be placed in the correct directory structure:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">professor.jpg</span></code>: An image file containing a clear picture of the “professor’s” face. This must be in the same directory as the script.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vosk-model-small-en-us-0.15/</span></code>: The Vosk speech recognition model directory. This must be in the same directory as the script.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model/keypoint_classifier/keypoint_classifier_weights.pth</span></code>: The pre-trained PyTorch model weights for the hand gesture classifier.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model/keypoint_classifier/keypoint_classifier_label.csv</span></code>: A CSV file containing the labels corresponding to the gesture classifier’s output indices.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model/keypoint_classifier/keypoint.csv</span></code>: A CSV file used for logging new hand gesture data.</p></li>
</ul>
<blockquote>
<div><p>Command-line Arguments</p>
</div></blockquote>
<hr class="docutils" />
<p>The script accepts several optional command-line arguments:</p>
<dl class="std option">
<dt class="sig sig-object std" id="cmdoption-device">
<span class="sig-name descname"><span class="pre">--device</span></span><span class="sig-prename descclassname"> <span class="pre">&lt;int&gt;</span></span><a class="headerlink" href="#cmdoption-device" title="Permalink to this definition"></a></dt>
<dd><p>Specifies the camera device ID. Default is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
</dd></dl>

<dl class="std option">
<dt class="sig sig-object std" id="cmdoption-width">
<span class="sig-name descname"><span class="pre">--width</span></span><span class="sig-prename descclassname"> <span class="pre">&lt;int&gt;</span></span><a class="headerlink" href="#cmdoption-width" title="Permalink to this definition"></a></dt>
<dd><p>Sets the capture width of the camera. Default is <code class="docutils literal notranslate"><span class="pre">960</span></code>.</p>
</dd></dl>

<dl class="std option">
<dt class="sig sig-object std" id="cmdoption-height">
<span class="sig-name descname"><span class="pre">--height</span></span><span class="sig-prename descclassname"> <span class="pre">&lt;int&gt;</span></span><a class="headerlink" href="#cmdoption-height" title="Permalink to this definition"></a></dt>
<dd><p>Sets the capture height of the camera. Default is <code class="docutils literal notranslate"><span class="pre">540</span></code>.</p>
</dd></dl>

<dl class="std option">
<dt class="sig sig-object std" id="cmdoption-use_static_image_mode">
<span class="sig-name descname"><span class="pre">--use_static_image_mode</span></span><span class="sig-prename descclassname"></span><a class="headerlink" href="#cmdoption-use_static_image_mode" title="Permalink to this definition"></a></dt>
<dd><p>A flag to indicate that MediaPipe should treat the video feed as a series of unrelated images.</p>
</dd></dl>

<dl class="std option">
<dt class="sig sig-object std" id="cmdoption-min_detection_confidence">
<span class="sig-name descname"><span class="pre">--min_detection_confidence</span></span><span class="sig-prename descclassname"> <span class="pre">&lt;float&gt;</span></span><a class="headerlink" href="#cmdoption-min_detection_confidence" title="Permalink to this definition"></a></dt>
<dd><p>Sets the minimum confidence value (from 0.0 to 1.0) for the hand detection to be considered successful. Default is <code class="docutils literal notranslate"><span class="pre">0.7</span></code>.</p>
</dd></dl>

<dl class="std option">
<dt class="sig sig-object std" id="cmdoption-min_tracking_confidence">
<span class="sig-name descname"><span class="pre">--min_tracking_confidence</span></span><span class="sig-prename descclassname"> <span class="pre">&lt;float&gt;</span></span><a class="headerlink" href="#cmdoption-min_tracking_confidence" title="Permalink to this definition"></a></dt>
<dd><p>Sets the minimum confidence value (from 0.0 to 1.0) for the hand landmarks to be considered tracked successfully. Default is <code class="docutils literal notranslate"><span class="pre">0.5</span></code>.</p>
</dd></dl>

<section id="architecture-program-flow">
<h3>Architecture &amp; Program Flow<a class="headerlink" href="#architecture-program-flow" title="Permalink to this heading"></a></h3>
<p>The application operates as a state machine, managed by the <code class="docutils literal notranslate"><span class="pre">current_mode</span></code> variable within the main loop.</p>
</section>
</section>
<section id="state-1-searching-professor-initial">
<h2>State 1: <cite>SEARCHING_PROFESSOR_INITIAL</cite><a class="headerlink" href="#state-1-searching-professor-initial" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>This is the initial state upon startup.</p></li>
<li><p>The system captures frames and resizes them by 50% for faster processing.</p></li>
<li><p>It uses the <cite>face_recognition</cite> library to detect all faces in the down-scaled frame.</p></li>
<li><p>For each detected face, it compares its encoding to the pre-loaded encoding from <cite>professor.jpg</cite>.</p></li>
<li><p>If a match is found (with a tolerance of 0.55), the application stores the face’s bounding box, prints “Professor Found!”, and transitions to the <cite>TRACKING_PROFESSOR</cite> state.</p></li>
</ul>
</section>
<section id="state-2-tracking-professor">
<h2>State 2: <cite>TRACKING_PROFESSOR</cite><a class="headerlink" href="#state-2-tracking-professor" title="Permalink to this heading"></a></h2>
<p>This is the primary operational state where gesture and speech recognition occur.</p>
<ol class="arabic simple">
<li><p><strong>Pose ROI Calculation</strong>: It defines a large Region of Interest (ROI) around the professor’s last known face position. This dramatically reduces the processing load as pose detection is only run on this smaller section of the image.</p></li>
<li><p><strong>Pose Estimation</strong>: MediaPipe Pose is run on the ROI.</p></li>
<li><p><strong>Tracking &amp; Face Box Update</strong>:
* If a pose is successfully detected, the system uses the stable positions of the nose and eye landmarks from the <em>pose</em> model to update the face bounding box. This is more robust than re-running face detection.
* If the pose is lost (e.g., the professor walks off-screen), the system transitions to the <cite>REACQUIRING_PROFESSOR</cite> state.</p></li>
<li><p><strong>Hand ROI Calculation</strong>: It checks the visibility of the left wrist landmark from the pose model. If visible, it creates a <em>new, smaller, dynamic ROI</em> centered on the wrist’s position.</p></li>
<li><p><strong>Hand Landmark Detection</strong>: MediaPipe Hands is run on this hand-specific ROI.</p></li>
<li><p><strong>Gesture Classification</strong>:
* If a hand is detected, its 21 landmarks are extracted.
* The landmarks are pre-processed (normalized relative to the wrist) and fed into the <cite>KeyPointClassifier</cite>.
* The classifier returns a sign ID (e.g., 0 for ‘Open’, 1 for ‘Start’, etc.).</p></li>
<li><p><strong>Action Execution</strong>: Based on the gesture ID, and subject to a <cite>1.5 second</cite> cooldown, a <cite>pyautogui</cite> action is triggered.</p></li>
</ol>
</section>
<section id="state-3-reacquiring-professor">
<h2>State 3: <cite>REACQUIRING_PROFESSOR</cite><a class="headerlink" href="#state-3-reacquiring-professor" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>This state is entered if pose tracking is lost.</p></li>
<li><p>The logic is identical to the initial search state, but it displays a “RE-ACQUIRING” message.</p></li>
<li><p>Once the professor is found again, it returns to the <cite>TRACKING_PROFESSOR</cite> state.</p></li>
</ul>
</section>
<section id="speech-recognition-sub-system">
<h2>Speech Recognition Sub-System<a class="headerlink" href="#speech-recognition-sub-system" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>Speech recognition is controlled by the <cite>is_speech_mode_active</cite> boolean flag.</p></li>
<li><p>This flag is set to <cite>True</cite> by a specific hand gesture (ID 0, “Open”).</p></li>
<li><p>When active, the system reads audio chunks from the microphone in a non-blocking manner within the main video loop.</p></li>
<li><p>The audio is fed to the <cite>Vosk</cite> recognizer. When a complete phrase is recognized, the result is passed to <cite>execute_speech_action</cite>.</p></li>
<li><p><cite>get_keyword</cite> searches the recognized text for keywords (“next”, “previous”, “quit”) and their aliases.</p></li>
<li><p>If a keyword is found, a <cite>pyautogui</cite> action is triggered, subject to its own <cite>1.5 second</cite> cooldown.</p></li>
<li><p>Speech mode automatically deactivates after 5 seconds of no new speech-activating gestures to prevent accidental command execution.</p></li>
</ul>
<section id="module-function-reference">
<h3>Module &amp; Function Reference<a class="headerlink" href="#module-function-reference" title="Permalink to this heading"></a></h3>
<p>This section details the key functions within the script.</p>
</section>
</section>
<section id="speech-recognition">
<h2>Speech Recognition<a class="headerlink" href="#speech-recognition" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="get_keyword">
<span class="sig-name descname"><span class="pre">get_keyword</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></span><a class="headerlink" href="#get_keyword" title="Permalink to this definition"></a></dt>
<dd><p>Parses a string to find pre-defined command keywords or their aliases.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>text</strong> – The input string from the speech recognizer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A canonical keyword (‘next’, ‘previous’, ‘quit’) or <code class="docutils literal notranslate"><span class="pre">None</span></code> if no keyword is found.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="execute_speech_action">
<span class="sig-name descname"><span class="pre">execute_speech_action</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recognized_text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#execute_speech_action" title="Permalink to this definition"></a></dt>
<dd><p>Executes a keyboard command based on recognized speech. Enforces a 1.5-second cooldown between actions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>recognized_text</strong> – The text output from the Vosk recognizer.</p>
</dd>
<dt class="field-even">Global is_speech_mode_active<span class="colon">:</span></dt>
<dd class="field-even"><p>Only runs if this is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="image-processing-landmark-calculation">
<h2>Image Processing &amp; Landmark Calculation<a class="headerlink" href="#image-processing-landmark-calculation" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="calc_bounding_rect">
<span class="sig-name descname"><span class="pre">calc_bounding_rect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">landmarks</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span></span></span><a class="headerlink" href="#calc_bounding_rect" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the bounding box coordinates for a set of MediaPipe landmarks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image</strong> – The source image.</p></li>
<li><p><strong>landmarks</strong> – The MediaPipe landmarks object.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of four integers: <code class="docutils literal notranslate"><span class="pre">[x_min,</span> <span class="pre">y_min,</span> <span class="pre">x_max,</span> <span class="pre">y_max]</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="calc_landmark_list">
<span class="sig-name descname"><span class="pre">calc_landmark_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">landmarks</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span></span></span><a class="headerlink" href="#calc_landmark_list" title="Permalink to this definition"></a></dt>
<dd><p>Converts MediaPipe landmark objects into a list of pixel coordinates.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image</strong> – The source image.</p></li>
<li><p><strong>landmarks</strong> – The MediaPipe landmarks object.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of <code class="docutils literal notranslate"><span class="pre">[x,</span> <span class="pre">y]</span></code> coordinate pairs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pre_process_landmark">
<span class="sig-name descname"><span class="pre">pre_process_landmark</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">landmark_list</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span></span></span><a class="headerlink" href="#pre_process_landmark" title="Permalink to this definition"></a></dt>
<dd><p>Normalizes a list of landmark coordinates for the gesture classifier. It makes the landmarks relative to the first point (the wrist) and scales them by the maximum absolute value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>landmark_list</strong> – A list of landmark coordinates from <code class="docutils literal notranslate"><span class="pre">calc_landmark_list</span></code>.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A flattened list of normalized floating-point values.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="data-logging-debug-drawing">
<h2>Data Logging &amp; Debug Drawing<a class="headerlink" href="#data-logging-debug-drawing" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="logging_csv">
<span class="sig-name descname"><span class="pre">logging_csv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">number</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">landmark_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">point_history_list</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#logging_csv" title="Permalink to this definition"></a></dt>
<dd><p>Saves the processed landmark list to a CSV file for training the gesture classifier. This is active only when <code class="docutils literal notranslate"><span class="pre">mode</span></code> is 1 (Logging Key Point).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>number</strong> – The class label (0-9) for the gesture being logged.</p></li>
<li><p><strong>mode</strong> – The current application mode.</p></li>
<li><p><strong>landmark_list</strong> – The normalized landmark list to be saved.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="draw_landmarks">
<span class="sig-name descname"><span class="pre">draw_landmarks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">landmark_point</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">image</span></span></span><a class="headerlink" href="#draw_landmarks" title="Permalink to this definition"></a></dt>
<dd><p>Draws stylized landmarks and connections on the image.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="draw_info_text">
<span class="sig-name descname"><span class="pre">draw_info_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">brect</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">handedness</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hand_sign_text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">finger_gesture_text</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">image</span></span></span><a class="headerlink" href="#draw_info_text" title="Permalink to this definition"></a></dt>
<dd><p>Draws the recognized hand sign label above the hand’s bounding box.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="draw_info">
<span class="sig-name descname"><span class="pre">draw_info</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">number</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">image</span></span></span><a class="headerlink" href="#draw_info" title="Permalink to this definition"></a></dt>
<dd><p>Draws general application status information (FPS, Mode) on the screen.</p>
</dd></dl>

</section>
<section id="main-application">
<h2>Main Application<a class="headerlink" href="#main-application" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="main">
<span class="sig-name descname"><span class="pre">main</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#main" title="Permalink to this definition"></a></dt>
<dd><p>The main entry point of the application. It handles initialization of all components (camera, models, etc.), contains the main application loop, manages state transitions, and orchestrates calls to all other processing and drawing functions.</p>
</dd></dl>

<section id="modes-command-reference">
<h3>Modes &amp; Command Reference<a class="headerlink" href="#modes-command-reference" title="Permalink to this heading"></a></h3>
</section>
</section>
<section id="in-app-controls">
<h2>In-App Controls<a class="headerlink" href="#in-app-controls" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>‘q’ or ‘ESC’ Key</strong>: Quit the application.</p></li>
<li><p><strong>‘d’ Key</strong>: Toggle Debug Mode. This shows/hides the pose/hand ROIs, landmarks, and other visual aids.</p></li>
<li><p><strong>‘k’ Key</strong>: Switch to “Logging Key Point” mode.</p></li>
<li><p><strong>‘n’ Key</strong>: Switch back to “Normal” mode.</p></li>
<li><p><strong>‘0’-‘9’ Keys</strong>: When in Logging mode, logs the current hand gesture under the pressed number as a label.</p></li>
</ul>
</section>
<section id="gesture-voice-commands">
<h2>Gesture &amp; Voice Commands<a class="headerlink" href="#gesture-voice-commands" title="Permalink to this heading"></a></h2>
<p>The following table lists the recognized gestures and voice commands and their corresponding actions.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="usage.html" class="btn btn-neutral float-left" title="Usage Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, BaquaAbdellah &amp; HachimBoua, Supervised by Mr.Masrour, ENSAM .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>